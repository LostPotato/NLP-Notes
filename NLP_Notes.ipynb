{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE37gHpIFzaD"
      },
      "source": [
        "# This is a notebook that contains notes and example code on NLP \n",
        "\n",
        "Project ideas:\n",
        "Sentiment analysis of historical wikipeda articles\n",
        "* Explain the limitations and bias within the model\n",
        "* Use this as a way of showing the importance of how a model is trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeLwwZ0thMRm"
      },
      "source": [
        "# PREPROCESSING FUNCTION BLOCK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIxwaqI3hMAq",
        "outputId": "2d4872b9-e35d-4458-b511-73ab5662d7c6"
      },
      "source": [
        "import nltk, re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "%load_ext google.colab.data_table\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "normalizer = WordNetLemmatizer()\n",
        "\n",
        "def get_part_of_speech(word):\n",
        "  probable_part_of_speech = wordnet.synsets(word)\n",
        "  pos_counts = Counter()\n",
        "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
        "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
        "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
        "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
        "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
        "  return most_likely_part_of_speech\n",
        "\n",
        "def preprocess_text(text, stop_words_remove = False, join = False):\n",
        "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
        "  tokenized = word_tokenize(cleaned)\n",
        "  if stop_words_remove is True:\n",
        "    st = [words for words in tokenized if words not in stop_words]\n",
        "    tokenized = st\n",
        "  if join is True:\n",
        "      normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized])\n",
        "  else:\n",
        "     normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
        "  return normalized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The google.colab.data_table extension is already loaded. To reload it, use:\n",
            "  %reload_ext google.colab.data_table\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAi9nKE7GA_y"
      },
      "source": [
        "# Stemming\n",
        "Stemming is the process of reducing a word to its base form by removing prefixs and suffixes. Example: stemming the word \"***going***\" would give you \"***go***\"\n",
        "\n",
        "NLTK  package uses PorterStemmer,  the mode most often used is NLTK extensions\n",
        "\n",
        "## USEFUL WHEN:\n",
        "* When there is a bunch of different forms of the word but you are interested in just the how many times the base is used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjVJPkjqFyQ_",
        "outputId": "bd45c192-2e79-4924-a316-f0a2126f085f"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "# making a stemmer object\n",
        "stemmer = PorterStemmer()\n",
        "# text to stem\n",
        "populated_island = 'Java is an Indonesian island in the Pacific Ocean. It is the most populated island in the world, with over 140 million people.'\n",
        "# tokenizing the word\n",
        "island_tokenized = word_tokenize(populated_island)\n",
        "print(f\"tokenized:{ island_tokenized}\")\n",
        "# stemmed version\n",
        "stemmed = [stemmer.stem(token) for token in island_tokenized]\n",
        "print(f\"stemmed: {stemmed}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "tokenized:['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
            "stemmed: ['java', 'is', 'an', 'indonesian', 'island', 'in', 'the', 'pacif', 'ocean', '.', 'It', 'is', 'the', 'most', 'popul', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'peopl', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRLOZenJOlSm"
      },
      "source": [
        "# Getting part of speech\n",
        "\n",
        "--- Load in before running the lemmatization example --\n",
        "\n",
        "A non-optimized way of obtaining the part of speech by looking at synomys\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLAc6LsnO0Fk"
      },
      "source": [
        "# defining a method to get the part of speech for each word\n",
        "def get_part_of_speech(word):\n",
        "  # generates the a list of synonyms to establish context for part of speech\n",
        "  probable_part_of_speech = wordnet.synsets(word)\n",
        "  # making  a counter\n",
        "  pos_counts = Counter()\n",
        "  # Counting the parts of speech of each synonym in the list\n",
        "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
        "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
        "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
        "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
        "  # returning the most comon part of speech in the data set\n",
        "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
        "  return most_likely_part_of_speech"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frZJ9hy5xvQ2"
      },
      "source": [
        "## NLTK  method of taggin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OobauzXUx3Cx",
        "outputId": "c0facac3-ae06-4ca1-d1a3-56d31d0679db"
      },
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# text to analyze\n",
        "text = \"Wow! Ramona and her class are happily studying the new textbook she has on NLP.\"\n",
        "# tokenize text\n",
        "token_text = word_tokenize(text)\n",
        "# pass the tokenized text into pos_tag function (takes in full sentences of tokenized words)\n",
        "pos_text = pos_tag(token_text)\n",
        "# this returns a list of each token and its part of speech\n",
        "print(pos_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Wow', 'NN'), ('!', '.'), ('Ramona', 'NNP'), ('and', 'CC'), ('her', 'PRP$'), ('class', 'NN'), ('are', 'VBP'), ('happily', 'RB'), ('studying', 'VBG'), ('the', 'DT'), ('new', 'JJ'), ('textbook', 'NN'), ('she', 'PRP'), ('has', 'VBZ'), ('on', 'IN'), ('NLP', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38z_FitGI4B3"
      },
      "source": [
        "#Lemmatization\n",
        "Lemmatization is the process of casting the word into its root form. It is more involved then stemming because the model needs to also understand the part of speech, but is a little bit more accurate then stemming\n",
        "\n",
        "## USEFUL WHEN:\n",
        "You want to analyze the part of speech of a word\n",
        "\n",
        "## Limitations:\n",
        "You have to know the part of speech before hand"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0wuvxvmJkkm",
        "outputId": "9a1fcff4-97c2-456f-ee2c-3f3f611245d3"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from collections import Counter\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# -- LEMMENTING -- #\n",
        "# lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "populated_island = 'Indonesia was founded in 1945. It contains the most populated island in the world, Java, with over 140 million people.'\n",
        "# tokenizing the word\n",
        "populated_tokenized = word_tokenize(populated_island)\n",
        "print(f\"tokenized:{ island_tokenized}\")\n",
        "#  printing before part of speech optimization\n",
        "san_lemmatized = [lemmatizer.lemmatize(token) for token in populated_tokenized]\n",
        "print(f\"No part of speech lemmatized: {san_lemmatized}\")\n",
        "# lemmatized version -- lemmatize is taking in the word and part of speech of the word\n",
        "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in populated_tokenized]\n",
        "print(f\"lemmatized: {lemmatized}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "tokenized:['Java', 'is', 'an', 'Indonesian', 'island', 'in', 'the', 'Pacific', 'Ocean', '.', 'It', 'is', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
            "No part of speech lemmatized: ['Indonesia', 'wa', 'founded', 'in', '1945', '.', 'It', 'contains', 'the', 'most', 'populated', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n",
            "lemmatized: ['Indonesia', 'be', 'found', 'in', '1945', '.', 'It', 'contain', 'the', 'most', 'populate', 'island', 'in', 'the', 'world', ',', 'Java', ',', 'with', 'over', '140', 'million', 'people', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyDJlkONbcw"
      },
      "source": [
        "# Full processing example\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNmi4fGVNgt9",
        "outputId": "2058056b-47cf-4477-f986-fa7a779bc1a5"
      },
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "# loading model\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# text to break down\n",
        "oprah_wiki = '<p>Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville\\'s WLAC-TV. </p>'\n",
        "# cleaning the text\n",
        "clean = re.sub(r'[\\<p>\\.\\/\\-]', \"\", oprah_wiki)\n",
        "print(clean)\n",
        "# Making the words lower cased\n",
        "cl_lw = clean.lower()\n",
        "print(cl_lw)\n",
        "# tokenizing\n",
        "token_cl = word_tokenize(cl_lw)\n",
        "print(token_cl)\n",
        "# lemmenting\n",
        "lem_cl = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in token_cl]\n",
        "print(lem_cl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working in local media, she was both the youngest news anchor and the first black female news anchor at Nashville's WLACTV \n",
            "working in local media, she was both the youngest news anchor and the first black female news anchor at nashville's wlactv \n",
            "['working', 'in', 'local', 'media', ',', 'she', 'was', 'both', 'the', 'youngest', 'news', 'anchor', 'and', 'the', 'first', 'black', 'female', 'news', 'anchor', 'at', 'nashville', \"'s\", 'wlactv']\n",
            "['work', 'in', 'local', 'medium', ',', 'she', 'be', 'both', 'the', 'young', 'news', 'anchor', 'and', 'the', 'first', 'black', 'female', 'news', 'anchor', 'at', 'nashville', \"'s\", 'wlactv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7954z-NRQODb"
      },
      "source": [
        "# Parsing with Regular Expressions\n",
        "## Regex methods\n",
        "* **re.compile** --> defines a regex object that can be used to match patterns within text\n",
        "* **re.match** --> if the regex compile object exist, it matches whatever pattern you defined before hand, else, pass it the expression and then the text and it will work in the same fashion\n",
        "* **re.group** --> used to get the results of a match from the re object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjKUyq93Rjd3",
        "outputId": "3fac8a4a-b350-4b1d-85df-0f9930c581ee"
      },
      "source": [
        "import re\n",
        "\n",
        "# strings are defined\n",
        "character_1 = \"Dorothy was a cool person\"\n",
        "character_2 = \"Henry was not very nice\"\n",
        "\n",
        "# compile your regular expression here\n",
        "regular_expression = re.compile(\"\\w{7}\")\n",
        "\n",
        "# check for a match to character_1 here\n",
        "result_1 = regular_expression.match(character_1)\n",
        "\n",
        "# .group() holds the list of all the mathc expression\n",
        "match_1 = result_1.group(0)\n",
        "\n",
        "# printing the results\n",
        "print(match_1)\n",
        "\n",
        "# not defining an object, just using .match()\n",
        "result_2 = re.match(\"\\w{7}\", character_2)\n",
        "print(result_2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dorothy\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbo_jq-NTROQ"
      },
      "source": [
        "* **re.search** --> lookas at all the text and matches the first cases and not just the first word\n",
        "* **re.findall** --> looks at all the text and holds everything that matches the regex expression you pass it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo8AIGhZTbev",
        "outputId": "5825296d-452b-4bd5-bd13-4d689b944590"
      },
      "source": [
        "import re\n",
        "\n",
        "# defining the string to search\n",
        "text = \"Everything is green here, while in the country of the Munchkins blue was the favorite color. But the people do not seem to be as friendly as the Munchkins, and I'm afraid we shall be unable to find a place to pass the night.\"\n",
        "\n",
        "# using search\n",
        "first_match  = re.search(\".ee.\", text)\n",
        "print(first_match)\n",
        "\n",
        "# using findall\n",
        "all_match = re.findall('.ee', text)\n",
        "print(f\"All the matchs of 'ee': {all_match}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<re.Match object; span=(15, 19), match='reen'>\n",
            "All the matchs of 'ee': ['ree', 'see']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgzB7M-Z0iul"
      },
      "source": [
        "## Chunking\n",
        "A process of grouping words by their respective part of speech \n",
        "\n",
        "-- **USES** --\n",
        "--\n",
        "This process lets you look at the structure of the sentence based on how the parts of speech are being used\n",
        "\n",
        "**NOTE** [Consult ](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)this list on the tags for the different parts of speech\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6xQAyCO1Hck",
        "outputId": "7a4c206f-2c4f-4b84-dbac-b509a6a5ee3f"
      },
      "source": [
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import RegexpParser, Tree\n",
        "import re\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# -- GETTING A POS TAGGED SENTENCE -- #\n",
        "\n",
        "# text to analyze -- credit to tommy orange\n",
        "text = \"It’s important that he dress like an Indian, dance like an Indian, even if it is an act, even if he feels like a fraud the whole time, because the only way to be Indian in this world is to look and act like an Indian.\"\n",
        "# tokenize text\n",
        "token_text = word_tokenize(text)\n",
        "# pass the tokenized text into pos_tag function (takes in full sentences of tokenized words)\n",
        "pos_text = pos_tag(token_text)\n",
        "\n",
        "# -- CHUNKER -- #\n",
        "\n",
        "# defining our chuncker -- looking for places where an adjective and noun are grouped together -> AN\n",
        "chunk_grammar = \"AN: {<JJ><NN>}\"\n",
        "# RegexpParser object -- part of NLTK \n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "# chunking the text\n",
        "chunked_sentence = chunk_parser.parse(pos_text)\n",
        "# using NLTK tree function to view the structure\n",
        "Tree.fromstring(str(chunked_sentence)).pretty_print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[('It', 'PRP'), ('’', 'VBZ'), ('s', 'JJ'), ('important', 'JJ'), ('that', 'IN'), ('he', 'PRP'), ('dress', 'VBZ'), ('like', 'IN'), ('an', 'DT'), ('Indian', 'JJ'), (',', ','), ('dance', 'NN'), ('like', 'IN'), ('an', 'DT'), ('Indian', 'JJ'), (',', ','), ('even', 'RB'), ('if', 'IN'), ('it', 'PRP'), ('is', 'VBZ'), ('an', 'DT'), ('act', 'NN'), (',', ','), ('even', 'RB'), ('if', 'IN'), ('he', 'PRP'), ('feels', 'VBZ'), ('like', 'IN'), ('a', 'DT'), ('fraud', 'NN'), ('the', 'DT'), ('whole', 'JJ'), ('time', 'NN'), (',', ','), ('because', 'IN'), ('the', 'DT'), ('only', 'JJ'), ('way', 'NN'), ('to', 'TO'), ('be', 'VB'), ('Indian', 'JJ'), ('in', 'IN'), ('this', 'DT'), ('world', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('look', 'VB'), ('and', 'CC'), ('act', 'VB'), ('like', 'IN'), ('an', 'DT'), ('Indian', 'JJ'), ('.', '.')]\n",
            "                                                                                                                                                                                               S                                                                                                                                                                                                           \n",
            "   ____________________________________________________________________________________________________________________________________________________________________________________________|__________________________________________________________________________________________________________________________________________________________________________________________________          \n",
            "  |      |    |        |          |      |        |        |      |       |      |     |        |      |       |      |     |      |     |      |      |     |     |     |      |     |        |        |     |      |       |     |      |        |      |     |       |       |      |       |       |      |      |      |      |       |      |       |      |            AN                  AN       \n",
            "  |      |    |        |          |      |        |        |      |       |      |     |        |      |       |      |     |      |     |      |      |     |     |     |      |     |        |        |     |      |       |     |      |        |      |     |       |       |      |       |       |      |      |      |      |       |      |       |      |      ______|_____         _____|____     \n",
            "It/PRP ’/VBZ s/JJ important/JJ that/IN he/PRP dress/VBZ like/IN an/DT Indian/JJ ,/, dance/NN like/IN an/DT Indian/JJ ,/, even/RB if/IN it/PRP is/VBZ an/DT act/NN ,/, even/RB if/IN he/PRP feels/VBZ like/IN a/DT fraud/NN the/DT ,/, because/IN the/DT to/TO be/VB Indian/JJ in/IN this/DT world/NN is/VBZ to/TO look/VB and/CC act/VB like/IN an/DT Indian/JJ ./. whole/JJ     time/NN only/JJ     way/NN\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPM4X3hc6pE1"
      },
      "source": [
        "\n",
        "## Noun phrases\n",
        "These function as list of words that serve the purpose of being a now, formally defined as a group of words headed by a noun that includes modifers\n",
        "\n",
        "**Examples (bold is the noun)**\n",
        "* A **solider** in the window.\n",
        "* A fast, dazziling red, stupidly expensive **car**.\n",
        "\n",
        "The noun is pretty much modified by all of the words around it but essitally in just functing a noun for the purpose of meaning.\n",
        "\n",
        "Common forms in writing would be phrases that start with a DT(determiner, ie: a, the) followed by any number of adjectives with a noun at the end\n",
        "\n",
        "USES \n",
        "--\n",
        "When you are curious about:\n",
        "* **how often noun phrases occur**, \n",
        "* ***adjectives used to define a particular noun***, \n",
        "* **the length of noun phrases in particular styles**,\n",
        "* **decerning topics of sentences**, so much more! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sws_gqV9umt"
      },
      "source": [
        "import re\n",
        "tokenized_sentences =['Twitter declined to address the backlash it has faced for allowing members of the Taliban to have accounts on the service, but said it would continue its work monitoring content for any policy violations.', \"Zabihullah Mujahid, a spokesperson for the organization, shared live updates of the group's takeover of Afghanistan on its Twitter feed.\", 'Sharing claims that the public is \"happy\" with the group\\'s arrival, Twitter came under criticism for allowing the Taliban to spread its messages, especially after former President Donald Trump was banned.', 'When asked for comment on the backlash the social media platform was receiving, a Twitter spokesperson told Newsweek the company will \"continue to proactively enforce our rules and review content that may violate Twitter rules, specifically policies against glorification of violence, platform manipulation and spam.\"', 'On Tuesday, Representative Doug Lamborn sent a letter to Twitter CEO Jack Dorsey expressing \"concerns\" about members of the Taliban being allowed on the platform but not Trump.', 'He called it \"clear\" that the Taliban falls under the violent organization category and noted that spokespeople have been promoting messages of a peaceful takeover that runs contrary to media reports of violence against civilians.', '\"In my review of these accounts, I did not find a single fact check on any of their tweets, nor any warnings for false or misleading content,\" Lamborn wrote.', '\"It is impossible to see how the accounts of Zabihullah Mujahid and Yousef Ahmandi do not violate your policies.\"', 'An October 2020 update from Twitter noted that there is \"no place\" for violent organizations.', 'Assessments as to what constitutes a \"violent organization\" under the policy are \"informed by national and international terrorism designations\" and include organizations that identify as an extremist group, have engaged in or currently engage in violence to further their cause and target civilians.', \"The Office of the Director of National Intelligence's counterterrorism guide lists the Afghan Taliban as a terrorist group and the group was placed on a Treasury Department list of specially designated global terrorists.\", \"The Taliban's Pakistan-based branch was also designated a foreign terrorist organization in 2010, although the Taliban itself has never received the designation from the State Department.\", 'This may be why Twitter is allowing the accounts to remain active.', \"Affiliating with or promoting a terrorist organizations' illicit activities violates the policy, according to Twitter.\", \"Newsweek reached out to Twitter for clarification as to why the Taliban accounts didn't violate the violent organization policy but did not receive a response in time for publication.\", 'While Twitter has largely avoided the issue of Taliban accounts, Facebook announced on Tuesday it would continue its ban on Taliban content and that it dedicated a team of Afghan experts to monitor and remove content.', '\"The Taliban is sanctioned as a terrorist organization under U.S. law and we have banned them from our services under our Dangerous Organization policies.', 'This means we remove accounts maintained by or on behalf of the Taliban and prohibit praise, support, and representation of them,\" a Facebook spokesperson told the BBC.', \"Much of the criticism aimed at Twitter came from Trump's supporters, including Representative Marjorie Taylor Greene, who recently had her suspension lifted, and Turning Point USA founder Charlie Kirk.\", 'Trump was banned from Twitter on January 8 in the wake of the Capitol riot.', 'The company decided to permanently suspend the former president due to the risk of \"further incitement of violence.\"', 'Regardless of a person\\'s opinion on de-platforming, Donie O\\'Sullivan, a correspondent for CNN, noted that Trump being banned while the Taliban is not, exposes \"some big holes in the company\\'s policy.\"'] \n",
        "tokenized_sentences = [re.sub(\"[^\\w*\\'?\\s]\", '', sentences) for sentences in tokenized_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-YXokR5UDdW"
      },
      "source": [
        "# -- HANDY WAY OF COUNTING CHUCKS -- #\n",
        "from collections import Counter\n",
        "\n",
        "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
        "def np_chunk_counter(chunked_sentences, phrase):\n",
        "\n",
        "    # create a list to hold chunks\n",
        "    chunks = list()\n",
        "\n",
        "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
        "    if phrase is \"NP\":\n",
        "        for chunked_sentence in chunked_sentences:\n",
        "            for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "                chunks.append(tuple(subtree))\n",
        "    if phrase is \"VP\":\n",
        "        for chunked_sentence in chunked_sentences:\n",
        "            for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
        "                chunks.append(tuple(subtree))\n",
        "\n",
        "\n",
        "    # create a Counter object\n",
        "    chunk_counter = Counter()\n",
        "\n",
        "    # for-loop through the list of chunks\n",
        "    for chunk in chunks:\n",
        "        # increase counter of specific chunk by 1\n",
        "        chunk_counter[chunk] += 1\n",
        "\n",
        "    # return 30 most frequent chunks\n",
        "    return chunk_counter.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAN1Tc5q-sA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619bda7b-bbbd-4929-872a-b7f2e044c5ba"
      },
      "source": [
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# -- Chunk Calculations -- #\n",
        "\n",
        "# defining the chunk to look for \n",
        "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# creating the parser\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# tagging the part of speeches of each word\n",
        "words_tokened = [nltk.word_tokenize(sentences) for sentences in tokenized_sentences]\n",
        "pos_words = [pos_tag(sentences) for sentences in words_tokened]\n",
        "\n",
        "# defining a list to hold the results of the parser and parser each sentences\n",
        "parsed_np_text = [chunk_parser.parse(sentences) for sentences in pos_words]\n",
        "\n",
        "# looking at the most common NPs\n",
        "print(np_chunk_counter(parsed_np_text, \"NP\"))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[((('content', 'NN'),), 4), ((('violence', 'NN'),), 4), ((('the', 'DT'), ('group', 'NN')), 3), ((('the', 'DT'), ('backlash', 'NN')), 2), ((('platform', 'NN'),), 2), ((('spokesperson', 'NN'),), 2), ((('the', 'DT'), ('company', 'NN')), 2), ((('the', 'DT'), ('violent', 'JJ'), ('organization', 'NN')), 2), ((('the', 'DT'), ('policy', 'NN')), 2), ((('policy', 'NN'),), 2)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK1ZZII8RG_j"
      },
      "source": [
        "# Verb Phrasing\n",
        "\n",
        "These are very similar in nature to the noun phrases but instead give you a lot of insight in to how the action of a sentence is being described\n",
        "\n",
        "Usually two basic structure:\n",
        "1. Verb -- NP -- and optional adverb(RB)\n",
        "2. NP -- VB -- and optional adverb(RB)\n",
        "\n",
        "These are useful in the same information task as np and the structure for tagging them is very much the same\n",
        "\n",
        "\n",
        "Understanding the regex code: \n",
        "--\n",
        "`<DT>?<JJ>*<NN>` -- Looking for the structure of a noun phrase -- optional DT plus any number of adjectives followd by a now\n",
        "\n",
        "` <VB.*><RB.?>?` -- a verb followed by an optional form of an adverb \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS0Nr5lCUEtM",
        "outputId": "10d823bb-bced-4448-d14a-658a3c5eff03"
      },
      "source": [
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# -- Chunk Calculations -- #\n",
        "\n",
        "# defining the chunk to look for \n",
        "chunk_grammar = \"VP: {<VB.*><DT>?<JJ>*<NN><RB.?>?}\"\n",
        "\n",
        "\n",
        "# creating the parser\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# tagging the part of speeches of each word\n",
        "words_tokened = [nltk.word_tokenize(sentences) for sentences in tokenized_sentences]\n",
        "pos_words = [pos_tag(sentences) for sentences in words_tokened]\n",
        "\n",
        "# defining a list to hold the results of the parser and parser each sentences\n",
        "parsed_vp_text = [chunk_parser.parse(sentences) for sentences in pos_words]\n",
        "\n",
        "# looking at the most common NPs\n",
        "print(np_chunk_counter(parsed_vp_text, \"VB\"))\n",
        "# If there was a verb phrase then this would print it"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6AnU2YgcTU-"
      },
      "source": [
        "# Chunk Filtering\n",
        "The process of defining the chunks that you want by first selecting the sentence then removing the parts of speech you don't want\n",
        "\n",
        "USEFUL FOR:\n",
        "--\n",
        "When you know what specific parts of a sentence you want to look at, and the implementation is fairly similar to other methods of chunking. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt3xwXr3f2Ey",
        "outputId": "0381b4fd-215d-4a88-a042-2e2c8c55dadc"
      },
      "source": [
        "import nltk\n",
        "from nltk import RegexpParser\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import RegexpParser, Tree\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# -- Chunk Calculations -- #\n",
        "\n",
        "# defining the chunk to be a sentence \n",
        "grammar = \"Chunk: {<.*>+}\"\n",
        "\n",
        "\n",
        "# creating the parser\n",
        "parser = RegexpParser(grammar)\n",
        "\n",
        "# tagging the part of speeches of each word\n",
        "words_tokened = [nltk.word_tokenize(sentences) for sentences in tokenized_sentences]\n",
        "pos_words = [pos_tag(sentences) for sentences in words_tokened]\n",
        "\n",
        "# chunking the 3rd sentence in the text above\n",
        "parsed_text = [parser.parse(pos_words[2])]\n",
        "\n",
        "# looking at the most common \n",
        "print(parsed_text)\n",
        "\n",
        "# Making a chunk that defines np and removes any verbs or preps\n",
        "chunk_grammar = \"\"\"NP: {<.*>+}\n",
        "                       }<VB.?|IN>+{\"\"\"\n",
        "# defining the parser again\n",
        "chunk_parser = RegexpParser(chunk_grammar)\n",
        "\n",
        "# Applying the filter\n",
        "filter_sentence2 = chunk_parser.parse(pos_words[2])\n",
        "\n",
        "# printing the tree\n",
        "Tree.fromstring(str(filter_sentence2)).pretty_print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[Tree('S', [Tree('Chunk', [('Sharing', 'VBG'), ('claims', 'NNS'), ('that', 'IN'), ('the', 'DT'), ('public', 'NN'), ('is', 'VBZ'), ('happy', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('group', 'NN'), (\"'s\", 'POS'), ('arrival', 'JJ'), ('Twitter', 'NNP'), ('came', 'VBD'), ('under', 'IN'), ('criticism', 'NN'), ('for', 'IN'), ('allowing', 'VBG'), ('the', 'DT'), ('Taliban', 'NNP'), ('to', 'TO'), ('spread', 'VB'), ('its', 'PRP$'), ('messages', 'NNS'), ('especially', 'RB'), ('after', 'IN'), ('former', 'JJ'), ('President', 'NNP'), ('Donald', 'NNP'), ('Trump', 'NNP'), ('was', 'VBD'), ('banned', 'VBN')])])]\n",
            "                                                                                                                                                  S                                                                                                                                                                         \n",
            "      ____________________________________________________________________________________________________________________________________________|_____________________________________________________________________________________________________________________________________________                             \n",
            "     |         |      |       |       |        |       |         |           |        |        |        |          NP             NP              NP                      NP                               NP                  NP                         NP                                    NP                          \n",
            "     |         |      |       |       |        |       |         |           |        |        |        |          |         _____|______         |        _______________|____________________            |          _________|________        __________|_____________             ___________|______________________      \n",
            "Sharing/VBG that/IN is/VBZ with/IN came/VBD under/IN for/IN allowing/VBG spread/VB after/IN was/VBD banned/VBN claims/NNS the/DT     public/NN happy/JJ the/DT group/NN 's/POS arrival/JJ Twitter/NNP criticism/NN the/DT Taliban/NNP to/TO its/PRP$ messages/NNS especially/RB former/JJ President/NNP Donald/NNP Trump/NNP\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT24iYj0fts3"
      },
      "source": [
        "# Bag of Words\n",
        "\n",
        "It can be used for all sorts of applications such as:\n",
        "* topic analysis of songs\n",
        "* filtering out text\n",
        "* sentiment analysis\n",
        "* word clouds\n",
        "\n",
        "Basics: BoW models are most interested in word count and don't care about the order of the words. Also know as unigram model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6T3nYpjj3VJ"
      },
      "source": [
        "# Basic function for making BOW dictionary\n",
        "\n",
        "This will produce the frequency count for the words in the sentence as well as reduce them to the lowest root possible some words like **games** would be counted for **game**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcZdAYeAj9Z1",
        "outputId": "042a20cb-4c0c-4280-a03a-841af16576b0"
      },
      "source": [
        "def text_to_bow(some_text):\n",
        "  # create a blank dictionary\n",
        "  bow_dictionary = {}\n",
        "  # preprocess the text\n",
        "  tokens = preprocess_text(some_text)\n",
        "  # add to the count if its in the dictonary else add it to the dictionary\n",
        "  for token in tokens:\n",
        "    if token in bow_dictionary:\n",
        "      bow_dictionary[token] += 1\n",
        "    else:\n",
        "      bow_dictionary[token] = 1\n",
        "  return bow_dictionary\n",
        "\n",
        "# Example \n",
        "sentence ='Twitter declined to address the backlash it has faced for allowing members of the Taliban to have accounts on the service, but said it would continue its work monitoring content for any policy violations.'\n",
        "# Calling the funciton\n",
        "print(text_to_bow(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'twitter': 1, 'decline': 1, 'address': 1, 'backlash': 1, 'face': 1, 'allow': 1, 'member': 1, 'taliban': 1, 'account': 1, 'service': 1, 'say': 1, 'would': 1, 'continue': 1, 'work': 1, 'monitor': 1, 'content': 1, 'policy': 1, 'violation': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch4vuFwNqO-o"
      },
      "source": [
        "# BOW: Feature Extraction & Feature Dictionary\n",
        "\n",
        "Using the news article as an example, a feature dictionary will hold the index position of where the token appeared within all of the text\n",
        "\n",
        "Below is a pure python implementation of a feature dictionary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DToPUCtQrjfU",
        "outputId": "af379007-ccba-4b53-810c-b85368949556"
      },
      "source": [
        "# Define create_features_dictionary() below:\n",
        "def create_features_dictionary(documents):\n",
        "  # creating a dictonary to hold the index location of each words\n",
        "  features_dictionary = {}\n",
        "  # merging all of the sentences into a single string\n",
        "  merged = \" \".join(documents)\n",
        "  # processing the text\n",
        "  tokens = preprocess_text(merged)\n",
        "  # keeping track of the index position\n",
        "  index  = 0\n",
        "  # adding the index position as the key value if it doesnt have one\n",
        "  for token in tokens:\n",
        "    if token not in features_dictionary:\n",
        "      features_dictionary[token] = index\n",
        "      index += 1\n",
        "  # returning the arguments\n",
        "  return features_dictionary, tokens\n",
        "\n",
        "# Text to make into a feature dictionary\n",
        "tokenized_sentences =['Twitter declined to address the backlash it has faced for allowing members of the Taliban to have accounts on the service, but said it would continue its work monitoring content for any policy violations.', \"Zabihullah Mujahid, a spokesperson for the organization, shared live updates of the group's takeover of Afghanistan on its Twitter feed.\", 'Sharing claims that the public is \"happy\" with the group\\'s arrival, Twitter came under criticism for allowing the Taliban to spread its messages, especially after former President Donald Trump was banned.', 'When asked for comment on the backlash the social media platform was receiving, a Twitter spokesperson told Newsweek the company will \"continue to proactively enforce our rules and review content that may violate Twitter rules, specifically policies against glorification of violence, platform manipulation and spam.\"', 'On Tuesday, Representative Doug Lamborn sent a letter to Twitter CEO Jack Dorsey expressing \"concerns\" about members of the Taliban being allowed on the platform but not Trump.', 'He called it \"clear\" that the Taliban falls under the violent organization category and noted that spokespeople have been promoting messages of a peaceful takeover that runs contrary to media reports of violence against civilians.', '\"In my review of these accounts, I did not find a single fact check on any of their tweets, nor any warnings for false or misleading content,\" Lamborn wrote.', '\"It is impossible to see how the accounts of Zabihullah Mujahid and Yousef Ahmandi do not violate your policies.\"', 'An October 2020 update from Twitter noted that there is \"no place\" for violent organizations.', 'Assessments as to what constitutes a \"violent organization\" under the policy are \"informed by national and international terrorism designations\" and include organizations that identify as an extremist group, have engaged in or currently engage in violence to further their cause and target civilians.', \"The Office of the Director of National Intelligence's counterterrorism guide lists the Afghan Taliban as a terrorist group and the group was placed on a Treasury Department list of specially designated global terrorists.\", \"The Taliban's Pakistan-based branch was also designated a foreign terrorist organization in 2010, although the Taliban itself has never received the designation from the State Department.\", 'This may be why Twitter is allowing the accounts to remain active.', \"Affiliating with or promoting a terrorist organizations' illicit activities violates the policy, according to Twitter.\", \"Newsweek reached out to Twitter for clarification as to why the Taliban accounts didn't violate the violent organization policy but did not receive a response in time for publication.\", 'While Twitter has largely avoided the issue of Taliban accounts, Facebook announced on Tuesday it would continue its ban on Taliban content and that it dedicated a team of Afghan experts to monitor and remove content.', '\"The Taliban is sanctioned as a terrorist organization under U.S. law and we have banned them from our services under our Dangerous Organization policies.', 'This means we remove accounts maintained by or on behalf of the Taliban and prohibit praise, support, and representation of them,\" a Facebook spokesperson told the BBC.', \"Much of the criticism aimed at Twitter came from Trump's supporters, including Representative Marjorie Taylor Greene, who recently had her suspension lifted, and Turning Point USA founder Charlie Kirk.\", 'Trump was banned from Twitter on January 8 in the wake of the Capitol riot.', 'The company decided to permanently suspend the former president due to the risk of \"further incitement of violence.\"', 'Regardless of a person\\'s opinion on de-platforming, Donie O\\'Sullivan, a correspondent for CNN, noted that Trump being banned while the Taliban is not, exposes \"some big holes in the company\\'s policy.\"'] \n",
        "\n",
        "# running the function\n",
        "print(create_features_dictionary(tokenized_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "({'twitter': 0, 'decline': 1, 'address': 2, 'backlash': 3, 'face': 4, 'allow': 5, 'member': 6, 'taliban': 7, 'account': 8, 'service': 9, 'say': 10, 'would': 11, 'continue': 12, 'work': 13, 'monitor': 14, 'content': 15, 'policy': 16, 'violation': 17, 'zabihullah': 18, 'mujahid': 19, 'spokesperson': 20, 'organization': 21, 'share': 22, 'live': 23, 'update': 24, 'group': 25, 'takeover': 26, 'afghanistan': 27, 'fee': 28, 'claim': 29, 'public': 30, 'happy': 31, 'arrival': 32, 'come': 33, 'criticism': 34, 'spread': 35, 'message': 36, 'especially': 37, 'former': 38, 'president': 39, 'donald': 40, 'trump': 41, 'ban': 42, 'ask': 43, 'comment': 44, 'social': 45, 'medium': 46, 'platform': 47, 'receive': 48, 'tell': 49, 'newsweek': 50, 'company': 51, 'proactively': 52, 'enforce': 53, 'rule': 54, 'review': 55, 'may': 56, 'violate': 57, 'specifically': 58, 'glorification': 59, 'violence': 60, 'manipulation': 61, 'spam': 62, 'tuesday': 63, 'representative': 64, 'doug': 65, 'lamborn': 66, 'send': 67, 'letter': 68, 'ceo': 69, 'jack': 70, 'dorsey': 71, 'express': 72, 'concern': 73, 'call': 74, 'clear': 75, 'fall': 76, 'violent': 77, 'category': 78, 'note': 79, 'spokespeople': 80, 'promote': 81, 'peaceful': 82, 'run': 83, 'contrary': 84, 'report': 85, 'civilian': 86, 'find': 87, 'single': 88, 'fact': 89, 'check': 90, 'tweet': 91, 'warn': 92, 'false': 93, 'mislead': 94, 'write': 95, 'impossible': 96, 'see': 97, 'yousef': 98, 'ahmandi': 99, 'october': 100, '2020': 101, 'place': 102, 'assessment': 103, 'constitute': 104, 'inform': 105, 'national': 106, 'international': 107, 'terrorism': 108, 'designation': 109, 'include': 110, 'identify': 111, 'extremist': 112, 'engage': 113, 'currently': 114, 'cause': 115, 'target': 116, 'office': 117, 'director': 118, 'intelligence': 119, 'counterterrorism': 120, 'guide': 121, 'list': 122, 'afghan': 123, 'terrorist': 124, 'treasury': 125, 'department': 126, 'specially': 127, 'designate': 128, 'global': 129, 'pakistan': 130, 'base': 131, 'branch': 132, 'also': 133, 'foreign': 134, '2010': 135, 'although': 136, 'never': 137, 'state': 138, 'remain': 139, 'active': 140, 'affiliate': 141, 'illicit': 142, 'activity': 143, 'accord': 144, 'reach': 145, 'clarification': 146, 'response': 147, 'time': 148, 'publication': 149, 'largely': 150, 'avoid': 151, 'issue': 152, 'facebook': 153, 'announce': 154, 'dedicate': 155, 'team': 156, 'expert': 157, 'remove': 158, 'sanction': 159, 'u': 160, 'law': 161, 'dangerous': 162, 'mean': 163, 'maintain': 164, 'behalf': 165, 'prohibit': 166, 'praise': 167, 'support': 168, 'representation': 169, 'bbc': 170, 'much': 171, 'aim': 172, 'supporter': 173, 'marjorie': 174, 'taylor': 175, 'greene': 176, 'recently': 177, 'suspension': 178, 'lift': 179, 'turn': 180, 'point': 181, 'usa': 182, 'founder': 183, 'charlie': 184, 'kirk': 185, 'january': 186, '8': 187, 'wake': 188, 'capitol': 189, 'riot': 190, 'decide': 191, 'permanently': 192, 'suspend': 193, 'due': 194, 'risk': 195, 'incitement': 196, 'regardless': 197, 'person': 198, 'opinion': 199, 'de': 200, 'platforming': 201, 'donie': 202, 'sullivan': 203, 'correspondent': 204, 'cnn': 205, 'expose': 206, 'big': 207, 'hole': 208}, ['twitter', 'decline', 'address', 'backlash', 'face', 'allow', 'member', 'taliban', 'account', 'service', 'say', 'would', 'continue', 'work', 'monitor', 'content', 'policy', 'violation', 'zabihullah', 'mujahid', 'spokesperson', 'organization', 'share', 'live', 'update', 'group', 'takeover', 'afghanistan', 'twitter', 'fee', 'share', 'claim', 'public', 'happy', 'group', 'arrival', 'twitter', 'come', 'criticism', 'allow', 'taliban', 'spread', 'message', 'especially', 'former', 'president', 'donald', 'trump', 'ban', 'ask', 'comment', 'backlash', 'social', 'medium', 'platform', 'receive', 'twitter', 'spokesperson', 'tell', 'newsweek', 'company', 'continue', 'proactively', 'enforce', 'rule', 'review', 'content', 'may', 'violate', 'twitter', 'rule', 'specifically', 'policy', 'glorification', 'violence', 'platform', 'manipulation', 'spam', 'tuesday', 'representative', 'doug', 'lamborn', 'send', 'letter', 'twitter', 'ceo', 'jack', 'dorsey', 'express', 'concern', 'member', 'taliban', 'allow', 'platform', 'trump', 'call', 'clear', 'taliban', 'fall', 'violent', 'organization', 'category', 'note', 'spokespeople', 'promote', 'message', 'peaceful', 'takeover', 'run', 'contrary', 'medium', 'report', 'violence', 'civilian', 'review', 'account', 'find', 'single', 'fact', 'check', 'tweet', 'warn', 'false', 'mislead', 'content', 'lamborn', 'write', 'impossible', 'see', 'account', 'zabihullah', 'mujahid', 'yousef', 'ahmandi', 'violate', 'policy', 'october', '2020', 'update', 'twitter', 'note', 'place', 'violent', 'organization', 'assessment', 'constitute', 'violent', 'organization', 'policy', 'inform', 'national', 'international', 'terrorism', 'designation', 'include', 'organization', 'identify', 'extremist', 'group', 'engage', 'currently', 'engage', 'violence', 'cause', 'target', 'civilian', 'office', 'director', 'national', 'intelligence', 'counterterrorism', 'guide', 'list', 'afghan', 'taliban', 'terrorist', 'group', 'group', 'place', 'treasury', 'department', 'list', 'specially', 'designate', 'global', 'terrorist', 'taliban', 'pakistan', 'base', 'branch', 'also', 'designate', 'foreign', 'terrorist', 'organization', '2010', 'although', 'taliban', 'never', 'receive', 'designation', 'state', 'department', 'may', 'twitter', 'allow', 'account', 'remain', 'active', 'affiliate', 'promote', 'terrorist', 'organization', 'illicit', 'activity', 'violate', 'policy', 'accord', 'twitter', 'newsweek', 'reach', 'twitter', 'clarification', 'taliban', 'account', 'violate', 'violent', 'organization', 'policy', 'receive', 'response', 'time', 'publication', 'twitter', 'largely', 'avoid', 'issue', 'taliban', 'account', 'facebook', 'announce', 'tuesday', 'would', 'continue', 'ban', 'taliban', 'content', 'dedicate', 'team', 'afghan', 'expert', 'monitor', 'remove', 'content', 'taliban', 'sanction', 'terrorist', 'organization', 'u', 'law', 'ban', 'service', 'dangerous', 'organization', 'policy', 'mean', 'remove', 'account', 'maintain', 'behalf', 'taliban', 'prohibit', 'praise', 'support', 'representation', 'facebook', 'spokesperson', 'tell', 'bbc', 'much', 'criticism', 'aim', 'twitter', 'come', 'trump', 'supporter', 'include', 'representative', 'marjorie', 'taylor', 'greene', 'recently', 'suspension', 'lift', 'turn', 'point', 'usa', 'founder', 'charlie', 'kirk', 'trump', 'ban', 'twitter', 'january', '8', 'wake', 'capitol', 'riot', 'company', 'decide', 'permanently', 'suspend', 'former', 'president', 'due', 'risk', 'incitement', 'violence', 'regardless', 'person', 'opinion', 'de', 'platforming', 'donie', 'sullivan', 'correspondent', 'cnn', 'note', 'trump', 'ban', 'taliban', 'expose', 'big', 'hole', 'company', 'policy'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Tz2-oZ1Z4x"
      },
      "source": [
        "# Build the BOW Vector\n",
        "\n",
        "The next step in using the BOW language model, is building a vectored list. This will help you understand the relationship between these words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7v6Ka1C2RsI",
        "outputId": "615a11fa-f4ae-469a-a51e-a9b9f474fea8"
      },
      "source": [
        "# Single sentence bow vectorization\n",
        "def text_to_bow_vector(some_text, features_dictionary):\n",
        "  # defining the the vector to be returned giving each value +1 for the times it appears\n",
        "  # in the features dictionary \n",
        "  bow_vector = [0 for keys in features_dictionary]\n",
        "  # preprocessing the text\n",
        "  tokens = preprocess_text(some_text)\n",
        "  # adding a +1 if the feature is in the dictioary\n",
        "  for token in tokens:\n",
        "      if token in features_dictionary:\n",
        "        feature_index = features_dictionary[token]\n",
        "        bow_vector[feature_index] += 1\n",
        "  # returning both the bag of words and tokens\n",
        "  return bow_vector, tokens\n",
        "\n",
        "# -- showing the function in action -- #\n",
        "\n",
        "# Text to make into a feature dictionary\n",
        "tokenized_sentences =['Twitter declined to address the backlash it has faced for allowing members of the Taliban to have accounts on the service, but said it would continue its work monitoring content for any policy violations.', \"Zabihullah Mujahid, a spokesperson for the organization, shared live updates of the group's takeover of Afghanistan on its Twitter feed.\", 'Sharing claims that the public is \"happy\" with the group\\'s arrival, Twitter came under criticism for allowing the Taliban to spread its messages, especially after former President Donald Trump was banned.', 'When asked for comment on the backlash the social media platform was receiving, a Twitter spokesperson told Newsweek the company will \"continue to proactively enforce our rules and review content that may violate Twitter rules, specifically policies against glorification of violence, platform manipulation and spam.\"', 'On Tuesday, Representative Doug Lamborn sent a letter to Twitter CEO Jack Dorsey expressing \"concerns\" about members of the Taliban being allowed on the platform but not Trump.', 'He called it \"clear\" that the Taliban falls under the violent organization category and noted that spokespeople have been promoting messages of a peaceful takeover that runs contrary to media reports of violence against civilians.', '\"In my review of these accounts, I did not find a single fact check on any of their tweets, nor any warnings for false or misleading content,\" Lamborn wrote.', '\"It is impossible to see how the accounts of Zabihullah Mujahid and Yousef Ahmandi do not violate your policies.\"', 'An October 2020 update from Twitter noted that there is \"no place\" for violent organizations.', 'Assessments as to what constitutes a \"violent organization\" under the policy are \"informed by national and international terrorism designations\" and include organizations that identify as an extremist group, have engaged in or currently engage in violence to further their cause and target civilians.', \"The Office of the Director of National Intelligence's counterterrorism guide lists the Afghan Taliban as a terrorist group and the group was placed on a Treasury Department list of specially designated global terrorists.\", \"The Taliban's Pakistan-based branch was also designated a foreign terrorist organization in 2010, although the Taliban itself has never received the designation from the State Department.\", 'This may be why Twitter is allowing the accounts to remain active.', \"Affiliating with or promoting a terrorist organizations' illicit activities violates the policy, according to Twitter.\", \"Newsweek reached out to Twitter for clarification as to why the Taliban accounts didn't violate the violent organization policy but did not receive a response in time for publication.\", 'While Twitter has largely avoided the issue of Taliban accounts, Facebook announced on Tuesday it would continue its ban on Taliban content and that it dedicated a team of Afghan experts to monitor and remove content.', '\"The Taliban is sanctioned as a terrorist organization under U.S. law and we have banned them from our services under our Dangerous Organization policies.', 'This means we remove accounts maintained by or on behalf of the Taliban and prohibit praise, support, and representation of them,\" a Facebook spokesperson told the BBC.', \"Much of the criticism aimed at Twitter came from Trump's supporters, including Representative Marjorie Taylor Greene, who recently had her suspension lifted, and Turning Point USA founder Charlie Kirk.\", 'Trump was banned from Twitter on January 8 in the wake of the Capitol riot.', 'The company decided to permanently suspend the former president due to the risk of \"further incitement of violence.\"', 'Regardless of a person\\'s opinion on de-platforming, Donie O\\'Sullivan, a correspondent for CNN, noted that Trump being banned while the Taliban is not, exposes \"some big holes in the company\\'s policy.\"'] \n",
        "\n",
        "# running the feature dictionary function\n",
        "feat_dict, tokens = create_features_dictionary(tokenized_sentences[5:13:1])\n",
        "print(tokenized_sentences[5])\n",
        "# making a bow vector of the first sentence\n",
        "bow_vector, tokens = text_to_bow_vector(tokenized_sentences[0], feat_dict)\n",
        "\n",
        "# printing the bow_vector\n",
        "print(bow_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He called it \"clear\" that the Taliban falls under the violent organization category and noted that spokespeople have been promoting messages of a peaceful takeover that runs contrary to media reports of violence against civilians.\n",
            "Twitter declined to address the backlash it has faced for allowing members of the Taliban to have accounts on the service, but said it would continue its work monitoring content for any policy violations.\n",
            "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s54wia9jeO4"
      },
      "source": [
        "Understanding the output:\n",
        "The vector returned will indicate if the text given appears in the feature dictonary that you passed in. If it does, the count is stored at the index position of the dictionary that the token from the sentence appears at. So, for example, the word \"it\" appears in the feature dictonary at index 2, since \"it\" appears in the sentence we feed it, the count for it is increased by one!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFBHZpMhlRlB"
      },
      "source": [
        "# Using the Scikit for word vectors\n",
        "Scikit offers a very compact way of generating word vectors in just a few lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GE0VBx6jd-b",
        "outputId": "44bcee86-97a0-4333-e228-3d9d99ca1ece"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Import CountVectorizer from sklearn:\n",
        "from collections import Counter\n",
        "\n",
        "# This count vectorizer will give us a vector count object\n",
        "bow_vectorizer_count = CountVectorizer()\n",
        "\n",
        "# The text we are going to train the vector on\n",
        "tokenized_sentences =['Twitter declined to address the backlash it has faced for allowing members of the Taliban to have accounts on the service, but said it would continue its work monitoring content for any policy violations.', \"Zabihullah Mujahid, a spokesperson for the organization, shared live updates of the group's takeover of Afghanistan on its Twitter feed.\", 'Sharing claims that the public is \"happy\" with the group\\'s arrival, Twitter came under criticism for allowing the Taliban to spread its messages, especially after former President Donald Trump was banned.', 'When asked for comment on the backlash the social media platform was receiving, a Twitter spokesperson told Newsweek the company will \"continue to proactively enforce our rules and review content that may violate Twitter rules, specifically policies against glorification of violence, platform manipulation and spam.\"', 'On Tuesday, Representative Doug Lamborn sent a letter to Twitter CEO Jack Dorsey expressing \"concerns\" about members of the Taliban being allowed on the platform but not Trump.', 'He called it \"clear\" that the Taliban falls under the violent organization category and noted that spokespeople have been promoting messages of a peaceful takeover that runs contrary to media reports of violence against civilians.', '\"In my review of these accounts, I did not find a single fact check on any of their tweets, nor any warnings for false or misleading content,\" Lamborn wrote.', '\"It is impossible to see how the accounts of Zabihullah Mujahid and Yousef Ahmandi do not violate your policies.\"', 'An October 2020 update from Twitter noted that there is \"no place\" for violent organizations.', 'Assessments as to what constitutes a \"violent organization\" under the policy are \"informed by national and international terrorism designations\" and include organizations that identify as an extremist group, have engaged in or currently engage in violence to further their cause and target civilians.', \"The Office of the Director of National Intelligence's counterterrorism guide lists the Afghan Taliban as a terrorist group and the group was placed on a Treasury Department list of specially designated global terrorists.\", \"The Taliban's Pakistan-based branch was also designated a foreign terrorist organization in 2010, although the Taliban itself has never received the designation from the State Department.\", 'This may be why Twitter is allowing the accounts to remain active.', \"Affiliating with or promoting a terrorist organizations' illicit activities violates the policy, according to Twitter.\", \"Newsweek reached out to Twitter for clarification as to why the Taliban accounts didn't violate the violent organization policy but did not receive a response in time for publication.\", 'While Twitter has largely avoided the issue of Taliban accounts, Facebook announced on Tuesday it would continue its ban on Taliban content and that it dedicated a team of Afghan experts to monitor and remove content.', '\"The Taliban is sanctioned as a terrorist organization under U.S. law and we have banned them from our services under our Dangerous Organization policies.', 'This means we remove accounts maintained by or on behalf of the Taliban and prohibit praise, support, and representation of them,\" a Facebook spokesperson told the BBC.', \"Much of the criticism aimed at Twitter came from Trump's supporters, including Representative Marjorie Taylor Greene, who recently had her suspension lifted, and Turning Point USA founder Charlie Kirk.\", 'Trump was banned from Twitter on January 8 in the wake of the Capitol riot.', 'The company decided to permanently suspend the former president due to the risk of \"further incitement of violence.\"', 'Regardless of a person\\'s opinion on de-platforming, Donie O\\'Sullivan, a correspondent for CNN, noted that Trump being banned while the Taliban is not, exposes \"some big holes in the company\\'s policy.\"'] \n",
        "\n",
        "# Lets split this up into a training set and a testing set\n",
        "# train set is everything after the 3rd sentence in the article\n",
        "train_set = tokenized_sentences[3::]\n",
        "# test set will be the first 3 sentences\n",
        "test_set = tokenized_sentences[0:3:1]\n",
        "# Define training_vectors:\n",
        "bow_vectorizer = bow_vectorizer_count.fit(train_set)\n",
        "# Define test_vectors:\n",
        "bow_vectors = bow_vectorizer.transform(test_set)\n",
        "# printing them as arrays\n",
        "print(bow_vectors.toarray()[0])\n",
        "# The ouput is how many times the features of the all sentences afte the third appear\n",
        "# in the first third of the sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 3 0 0 0 0 0 0 2 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uBRMOxuoqot"
      },
      "source": [
        "# Looking at a Scikits implementations\n",
        "\n",
        "Check out this [link](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files) for some extremely nice examples of how to implement Scikits language pipeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTU0ABA9qZUv"
      },
      "source": [
        "# Term Frequency-Inverse Document Frequency\n",
        "\n",
        "Basic Idea: Getting an idea of which words are actually important in understanding the meaning of the sentence. It is also know as tf-idf\n",
        "\n",
        "How it works: you apply it over a large body of text containing multiple documents, the tf-idf gives you a score of each words importance relative to how many times it appears within each text document. This would make more common words have less of score if the appear across each document, but unique words would have a high score.\n",
        "\n",
        "Its a bag of words model that gives a better insight to what words are important relative to the document they are in."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS1IPqPQJwQo",
        "outputId": "e5e0f0de-cdf8-4689-db50-af045ef0bc6c"
      },
      "source": [
        "# Example \n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# -- Preprocessing function needs to join to be set to true! -- #\n",
        "\n",
        "# defining the text we want to look at\n",
        "document_1 = \"They ran across the small river\"\n",
        "document_2 = \"I ran across the small river\"\n",
        "document_3 = \"He ran across the small river\"\n",
        "\n",
        "# corpus of documents\n",
        "corpus = [document_1, document_2, document_3]\n",
        "\n",
        "# preprocess documents\n",
        "processed_corpus = [preprocess_text(doc, join = True) for doc in corpus]\n",
        "\n",
        "# initialize and fit TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(norm=None)\n",
        "tf_idf_scores = vectorizer.fit_transform(processed_corpus)\n",
        "\n",
        "# get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "corpus_index = [n for n in processed_corpus]\n",
        "\n",
        "# create pandas DataFrame with tf-idf scores\n",
        "df_tf_idf = pd.DataFrame(tf_idf_scores.T.todense(), index=feature_names, columns=corpus_index)\n",
        "print(df_tf_idf)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "        they run across the small river  ...  he run across the small river\n",
            "across                         1.000000  ...                       1.000000\n",
            "he                             0.000000  ...                       1.693147\n",
            "river                          1.000000  ...                       1.000000\n",
            "run                            1.000000  ...                       1.000000\n",
            "small                          1.000000  ...                       1.000000\n",
            "the                            1.000000  ...                       1.000000\n",
            "they                           1.693147  ...                       0.000000\n",
            "\n",
            "[7 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xhm7KNnNXC9"
      },
      "source": [
        "For these next two exercises poems by emily dickinson are used  to look at the application of tf-idf.\n",
        "\n",
        "### First exercise\n",
        "Looking at the first part of the TF - term frequency. We are getting an idea of how many times each word appears in the poem, the logic being that the more it appears the more relevence it should have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psKy9FRMPVQl",
        "outputId": "9d06f042-245a-4d2d-f8dd-451c96e483a2"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "poem ='''\n",
        "A Bird, came down the Walk -\n",
        "He did not know I saw -\n",
        "He bit an Angle Worm in halves\n",
        "And ate the fellow, raw,\n",
        "\n",
        "And then, he drank a Dew\n",
        "From a convenient Grass -\n",
        "And then hopped sidewise to the Wall\n",
        "To let a Beetle pass -\n",
        "\n",
        "He glanced with rapid eyes,\n",
        "That hurried all abroad -\n",
        "They looked like frightened Beads, I thought,\n",
        "He stirred his Velvet Head. -\n",
        "\n",
        "Like one in danger, Cautious,\n",
        "I offered him a Crumb,\n",
        "And he unrolled his feathers,\n",
        "And rowed him softer Home -\n",
        "\n",
        "Than Oars divide the Ocean,\n",
        "Too silver for a seam,\n",
        "Or Butterflies, off Banks of Noon,\n",
        "Leap, plashless as they swim.\n",
        "'''\n",
        "\n",
        "# preprocess text\n",
        "processed_poem = preprocess_text(poem, join=True)\n",
        "\n",
        "# initialize and fit CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "term_frequencies = vectorizer.fit_transform([processed_poem])\n",
        "\n",
        "# get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# create pandas DataFrame with term frequencies\n",
        "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=['Term Frequency'])\n",
        "print(df_term_frequencies)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Term Frequency\n",
            "abroad               1\n",
            "all                  1\n",
            "an                   1\n",
            "and                  5\n",
            "angle                1\n",
            "...                ...\n",
            "velvet               1\n",
            "walk                 1\n",
            "wall                 1\n",
            "with                 1\n",
            "worm                 1\n",
            "\n",
            "[79 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chcd4u1ARbvW"
      },
      "source": [
        "### Second exercise\n",
        "We can build on the features and information that we extracted in the last step\n",
        "\n",
        "This is the IDF part. This is a method of penalizing the words that appear the most often across **all** documents. This way super common words that don't provide insight across documentation should have a lower score.\n",
        "\n",
        "SciKit provides a super streamlined version of this. Link to the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVWgzlfFRbTK"
      },
      "source": [
        "# POEMS \n",
        "poem_1 = '''\n",
        "Success is counted sweetest\n",
        "By those who ne'er succeed.\n",
        "To comprehend a nectar\n",
        "Requires sorest need.\n",
        "\n",
        "Not one of all the purple host\n",
        "Who took the flag to-day\n",
        "Can tell the definition,\n",
        "So clear, of victory,\n",
        "\n",
        "As he, defeated, dying,\n",
        "On whose forbidden ear\n",
        "The distant strains of triumph\n",
        "Break, agonized and clear!'''\n",
        "\n",
        "poem_2 = '''\n",
        "Wild nights! Wild nights!\n",
        "Were I with thee,\n",
        "Wild nights should be\n",
        "Our luxury!\n",
        "\n",
        "Futile the winds\n",
        "To a heart in port, —\n",
        "Done with the compass,\n",
        "Done with the chart.\n",
        "\n",
        "Rowing in Eden!\n",
        "Ah! the sea!\n",
        "Might I but moor\n",
        "To-night in thee!'''\n",
        "\n",
        "poem_3 = '''\n",
        "I'm nobody! Who are you?\n",
        "Are you nobody, too?\n",
        "Then there 's a pair of us — don't tell!\n",
        "They 'd banish us, you know.\n",
        "\n",
        "How dreary to be somebody!\n",
        "How public, like a frog\n",
        "To tell your name the livelong day\n",
        "To an admiring bog!'''\n",
        "\n",
        "poem_4 = '''\n",
        "I felt a funeral in my brain,\n",
        "   And mourners, to and fro,\n",
        "Kept treading, treading, till it seemed\n",
        "   That sense was breaking through.\n",
        "\n",
        "And when they all were seated,\n",
        "   A service like a drum\n",
        "Kept beating, beating, till I thought\n",
        "   My mind was going numb.\n",
        "\n",
        "And then I heard them lift a box,\n",
        "   And creak across my soul\n",
        "With those same boots of lead, again.\n",
        "   Then space began to toll\n",
        "\n",
        "As all the heavens were a bell,\n",
        "   And Being but an ear,\n",
        "And I and silence some strange race,\n",
        "   Wrecked, solitary, here.'''\n",
        "\n",
        "poem_5 = '''\n",
        "Hope is the thing with feathers\n",
        "That perches in the soul,\n",
        "And sings the tune without the words,\n",
        "And never stops at all,\n",
        "\n",
        "And sweetest in the gale is heard;\n",
        "And sore must be the storm\n",
        "That could abash the little bird\n",
        "That kept so many warm.\n",
        "\n",
        "I 've heard it in the chillest land,\n",
        "And on the strangest sea;\n",
        "Yet, never, in extremity,\n",
        "It asked a crumb of me.'''\n",
        "\n",
        "poem_6 = '''\n",
        "The pedigree of honey\n",
        "Does not concern the bee;\n",
        "A clover, any time, to him\n",
        "Is aristocracy.'''\n",
        "\n",
        "poems = [poem_1, poem_2, poem_3, poem_4, poem_5, poem_6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TBnVxOpavQ0"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# -- Setting up the Term Frequencies -- #\n",
        "\n",
        "# process each poem\n",
        "poems_proce = [preprocess_text(poem, join=True) for poem in poems]\n",
        "\n",
        "# counting each word across the poems\n",
        "vectorizer = CountVectorizer()\n",
        "# Getting the total count\n",
        "term_frequencies = vectorizer.fit_transform(poems_proce)\n",
        "\n",
        "# Storing the words\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# createing the index for each poem\n",
        "corpus_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
        "\n",
        "# making a DataFrame for the term frequencies\n",
        "df_term_frequencies = pd.DataFrame(term_frequencies.T.todense(), index=feature_names, columns=corpus_index)\n",
        "\n",
        "# Printing just the term frequencies\n",
        "print(df_term_frequencies)\n",
        "\n",
        "# making the model and fitting it to the term frequencies\n",
        "transformer = TfidfTransformer()\n",
        "transformer.fit(term_frequencies)\n",
        "# grannomg the frequencies\n",
        "idf_values = transformer.idf_\n",
        "\n",
        "# making the df to store all of the inverse frequencies\n",
        "df_idf = pd.DataFrame(idf_values, index = feature_names, columns=['Inverse Document Frequency'])\n",
        "# printing the inverse\n",
        "print(df_idf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-Be9hfpm1NX"
      },
      "source": [
        "## Putting it together\n",
        "\n",
        "Overview of how the TF-IDF is calculated\n",
        "\n",
        "Tfidf (t,d) = tf(t,d) * idf(t,corpus)\n",
        "\n",
        "* *t* = term\n",
        "* *d* = document\n",
        "* *tf* = term frequency\n",
        "* *idf* = Inverse term frequency\n",
        "* *corpus* = all documentation\n",
        "\n",
        "Knowing how its calculated, we can understand the output form the scikit-learns TfidfVectorizer function. Note that this function auto normalizes the values of each word\n",
        "\n",
        "This function lets us pass in a string and it produces a vectorized version of the scores that we can then view with pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLVSeFmkoVHN"
      },
      "source": [
        "# -- Run the Poems block and preprocessing block -- #\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# preprocess documents\n",
        "processed_poems = [preprocess_text(poem, False,True) for poem in poems]\n",
        "\n",
        "# initialize the model\n",
        "vectorizer = TfidfVectorizer()\n",
        "# fit the model\n",
        "scores = vectorizer.fit_transform(processed_poems)\n",
        "\n",
        "# Storing the words\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# createing the index for each poem\n",
        "poem_index = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
        "\n",
        "# printing the TFIDF df\n",
        "df_tf_idf = pd.DataFrame(scores.T.todense(), index=feature_names, columns=poem_index)\n",
        "print(df_tf_idf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOr4VSAIqMQD"
      },
      "source": [
        "# Producing an TFIDF from a BOW model\n",
        "\n",
        "Lets say you alread have a BOW model and you want to convert that into TFIDF, this is pretty easily done with TfidfTransformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qev3NjTRqigx",
        "outputId": "2174c700-97af-41d1-d174-207dd2ba5f86"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# --  Making the BOW Model -- #\n",
        "\n",
        "# preprocess text\n",
        "processed_poems = [preprocess_text(poem, False,True) for poem in poems]\n",
        "\n",
        "# initialize and fit CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "BOW = vectorizer.fit_transform(processed_poems)\n",
        "\n",
        "# get vocabulary of terms\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "\n",
        "# Naming each poem column in the DF\n",
        "cindex = [f\"Poem {i+1}\" for i in range(len(poems))]\n",
        "\n",
        "# create pandas DataFrame with term frequencies\n",
        "df_term_frequencies = pd.DataFrame(BOW.T.todense(), index=feature_names, columns=cindex)\n",
        "\n",
        "# -- Transforming the BOW model -- #\n",
        "\n",
        "# defining the model\n",
        "transformer = TfidfTransformer(norm=None)\n",
        "# fitting the model to the BOW model\n",
        "scores = transformer.fit_transform(BOW)\n",
        "\n",
        "# making a DF for the values\n",
        "df_tf_idf = pd.DataFrame(scores.T.todense(), index=feature_names, columns=cindex)\n",
        "# printing the full df and the max value per poem\n",
        "print(df_tf_idf)\n",
        "print(df_tf_idf.max())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Poem 1  Poem 2    Poem 3    Poem 4    Poem 5  Poem 6\n",
            "abash    0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
            "across   0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
            "admire   0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
            "again    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
            "agonize  2.252763     0.0  0.000000  0.000000  0.000000     0.0\n",
            "...           ...     ...       ...       ...       ...     ...\n",
            "word     0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
            "wreck    0.000000     0.0  0.000000  2.252763  0.000000     0.0\n",
            "yet      0.000000     0.0  0.000000  0.000000  2.252763     0.0\n",
            "you      0.000000     0.0  6.758289  0.000000  0.000000     0.0\n",
            "your     0.000000     0.0  2.252763  0.000000  0.000000     0.0\n",
            "\n",
            "[173 rows x 6 columns]\n",
            "Poem 1     4.505526\n",
            "Poem 2     9.011052\n",
            "Poem 3     6.758289\n",
            "Poem 4    12.476926\n",
            "Poem 5     9.000000\n",
            "Poem 6     2.252763\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzSONUXvJNNB"
      },
      "source": [
        "# Word Emmbedings \n",
        "--- \n",
        "## Concept\n",
        "The concept stems from the idea that the words in a sentence gain there context from the other words around them. So basically the relationship between words is how we understand the words themselves\n",
        "\n",
        "## Implementation\n",
        "Words have been catagoriezed based on there relationship to each other into vectors, these vectors can hold an infinate amount of data points as it relates to some relationship the word has with other words. A popular package for doing this embedding is spacy package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-HCB9Q-NAvq"
      },
      "source": [
        "# Looking at word vectors and their length\n",
        "Spacy provides vectors on different languages so that you don't have to generate them yourself"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uigC7mzlsivv"
      },
      "source": [
        "# importing spacy\n",
        "import spacy\n",
        "\n",
        "# Loading the model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# Looking at different words\n",
        "sad_vector = nlp('sad').vector\n",
        "happy_vector = nlp('happy').vector\n",
        "angry_vector =nlp('angry').vector\n",
        "\n",
        "print(sad_vector[0:10], happy_vector[0:10], angry_vect0r[0:10])\n",
        "\n",
        "# Examining the length\n",
        "print(len(sad_vector))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsHWo1onQyhY"
      },
      "source": [
        "## How do we use this information?\n",
        "\n",
        "Distance, distance, distance. For the vectors to mean anything we need some way to quantify what they mean, and that can be done through a bunch of different ways, but the three most common are\n",
        "\n",
        "**Manhattan**  -- The simpliest where you compare the absolute diffrence between indexes and add them all up. Consider the vectors [1,2,3] and [2,4,6] then the distance would be: \n",
        "\n",
        "distance = ∣1−2∣+∣2−4∣+∣3−6∣=1+2+3=6\n",
        "\n",
        "**Euclidean** -- This is super close to the distance formula learned in math, you take the diffrence of each index squared then take the sum of all them and square root it. Using the same two vectors\n",
        "\n",
        "eu_distance = sqrt( diffrence 1^2 + diffrence 2^2 ... )\n",
        "\n",
        "**Cosine** -- This looks at the difference in terms of angle between the two vectors. Basically looking at which way the vectors point and caculating the difference between them. See [here](https://en.wikipedia.org/wiki/Cosine_similarity#Definition) for more info on the math\n",
        "\n",
        "## BUT WHY?\n",
        "\n",
        "The reason that the distance is so import is that in theroy words generate there meaning from the context they are in. Since the vector is generated from its context to other words, then similar words should have similar vectors. This gives a great insight into the context of a word\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC5hEBl4Tf2_",
        "outputId": "1a90afb3-1fd1-4fe4-bcab-3173f382285c"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cityblock, euclidean, cosine\n",
        "import spacy\n",
        "\n",
        "# load word embedding model\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "# define word embedding vectors\n",
        "happy_vec = nlp('happy').vector\n",
        "sad_vec = nlp('sad').vector\n",
        "angry_vec = nlp('angry').vector\n",
        "\n",
        "# calculate Manhattan distance\n",
        "man_happy_sad = cityblock(happy_vec, sad_vec)\n",
        "man_sad_angry = cityblock(sad_vec, angry_vec)\n",
        "print(\"manhattan distances: \",man_happy_sad, man_sad_angry)\n",
        "\n",
        "\n",
        "\n",
        "# calculate Euclidean distance\n",
        "euc_happy_sad = euclidean(happy_vec, sad_vec)\n",
        "euc_sad_angry = euclidean(sad_vec, angry_vec)\n",
        "print(\" Euclidean distances:\", euc_happy_sad, euc_sad_angry)\n",
        "\n",
        "# calculate cosine distance\n",
        "cos_happy_sad = cosine(happy_vec, sad_vec)\n",
        "cos_sad_angry = cosine(sad_vec, angry_vec)\n",
        "print(\"cosine distances: \", cos_happy_sad, cos_sad_angry)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "manhattan distances:  113.86418 118.5128\n",
            " Euclidean distances: 14.85196304321289 14.087335586547852\n",
            "cosine distances:  0.2744985818862915 0.23983347415924072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb72q8a4VYbq"
      },
      "source": [
        "##  How to generate the vectors\n",
        "\n",
        "### 1st Approach: CBOW\n",
        "To use CBOW or  continuous bag of words, this goes through each word in a training corupus and tries to predict the word that goes after it by applying the a bow model to the words around it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeG-XkqgVX8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854b3756-8d0e-458a-88ce-b5e0d63b7551"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# function for generating CBOWS based of a predeterimined context length\n",
        "# generating the example sentence and processing it\n",
        "sentence = \"To use CBOW or continuous bag of words, this goes through each word in a training corupus and tries to predict the word that goes after it by applying the a bow model to the words around it\"\n",
        "print(sentence)\n",
        "\n",
        "# preprocessing\n",
        "sentence_lst = [word.lower().strip(\".\") for word in sentence.split()]\n",
        "\n",
        "# context length\n",
        "context = 3\n",
        "\n",
        "def get_cbows(sentence_lst, context_length):\n",
        "  # defining the cbows array\n",
        "  cbows = []\n",
        "  # looping through the sentence list\n",
        "  for i, val in enumerate(sentence_lst):\n",
        "    # pass if the word has zero context around it\n",
        "    if i < context_length:\n",
        "      pass\n",
        "    # captures the context surrounding the word\n",
        "    elif i < len(sentence_lst) - context_length:\n",
        "      # generating a supset of the contextual words around the word of interest\n",
        "      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
        "      # generates a vector for each word\n",
        "      vectorizer = CountVectorizer()\n",
        "      vectorizer.fit_transform(context)\n",
        "      # getting the features of the vector\n",
        "      context_no_order = vectorizer.get_feature_names()\n",
        "      # appending the features and value of the vectors to the CBOWS list\n",
        "      cbows.append((val,context_no_order))\n",
        "  return cbows\n",
        "\n",
        "# running the function\n",
        "cbows = get_cbows(sentence_lst, context)\n",
        "# printing the results\n",
        "for cbow in cbows:\n",
        "    print(cbow)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To use CBOW or continuous bag of words, this goes through each word in a training corupus and tries to predict the word that goes after it by applying the a bow model to the words around it\n",
            "('or', ['bag', 'cbow', 'continuous', 'of', 'to', 'use'])\n",
            "('continuous', ['bag', 'cbow', 'of', 'or', 'use', 'words'])\n",
            "('bag', ['cbow', 'continuous', 'of', 'or', 'this', 'words'])\n",
            "('of', ['bag', 'continuous', 'goes', 'or', 'this', 'words'])\n",
            "('words,', ['bag', 'continuous', 'goes', 'of', 'this', 'through'])\n",
            "('this', ['bag', 'each', 'goes', 'of', 'through', 'words'])\n",
            "('goes', ['each', 'of', 'this', 'through', 'word', 'words'])\n",
            "('through', ['each', 'goes', 'in', 'this', 'word', 'words'])\n",
            "('each', ['goes', 'in', 'this', 'through', 'word'])\n",
            "('word', ['each', 'goes', 'in', 'through', 'training'])\n",
            "('in', ['corupus', 'each', 'through', 'training', 'word'])\n",
            "('a', ['and', 'corupus', 'each', 'in', 'training', 'word'])\n",
            "('training', ['and', 'corupus', 'in', 'tries', 'word'])\n",
            "('corupus', ['and', 'in', 'to', 'training', 'tries'])\n",
            "('and', ['corupus', 'predict', 'to', 'training', 'tries'])\n",
            "('tries', ['and', 'corupus', 'predict', 'the', 'to', 'training'])\n",
            "('to', ['and', 'corupus', 'predict', 'the', 'tries', 'word'])\n",
            "('predict', ['and', 'that', 'the', 'to', 'tries', 'word'])\n",
            "('the', ['goes', 'predict', 'that', 'to', 'tries', 'word'])\n",
            "('word', ['after', 'goes', 'predict', 'that', 'the', 'to'])\n",
            "('that', ['after', 'goes', 'it', 'predict', 'the', 'word'])\n",
            "('goes', ['after', 'by', 'it', 'that', 'the', 'word'])\n",
            "('after', ['applying', 'by', 'goes', 'it', 'that', 'word'])\n",
            "('it', ['after', 'applying', 'by', 'goes', 'that', 'the'])\n",
            "('by', ['after', 'applying', 'goes', 'it', 'the'])\n",
            "('applying', ['after', 'bow', 'by', 'it', 'the'])\n",
            "('the', ['applying', 'bow', 'by', 'it', 'model'])\n",
            "('a', ['applying', 'bow', 'by', 'model', 'the', 'to'])\n",
            "('bow', ['applying', 'model', 'the', 'to'])\n",
            "('model', ['bow', 'the', 'to', 'words'])\n",
            "('to', ['around', 'bow', 'model', 'the', 'words'])\n",
            "('the', ['around', 'bow', 'it', 'model', 'to', 'words'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9vYqOTJxTu5"
      },
      "source": [
        "### Approach 2.\n",
        "\n",
        "Using the skip grams across the text in order to gather context form how the words appear together. Think of it as interration and then using the patterns from the iteration to understand the positioning and how the words are used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_gyDuN3xTQo",
        "outputId": "e9ff4be8-e221-42ba-99ff-4b880576aa2b"
      },
      "source": [
        "# function for generating CBOWS based of a predeterimined context length\n",
        "# generating the example sentence and processing it\n",
        "sentence = \"To use CBOW or continuous bag of words, this goes through each word in a training corupus and tries to predict the word that goes after it by applying the a bow model to the words around it\"\n",
        "print(sentence)\n",
        "\n",
        "# preprocessing\n",
        "sentence_lst = [word.lower().strip(\".\") for word in sentence.split()]\n",
        "\n",
        "# context length\n",
        "context = 3\n",
        "\n",
        "# skips across the text and takes every third word and pairs it\n",
        "def get_skip_grams(sentence_lst, context_length):\n",
        "  # list to hold skip grams\n",
        "  skip_grams = []\n",
        "  \n",
        "# looping through the sentence list\n",
        "  for i, val in enumerate(sentence_lst):\n",
        "    # pass if the word has zero context around it\n",
        "    if i < context_length:\n",
        "      pass\n",
        "    # if there is enough context it starts to capture the words that appear every 3 position after the word\n",
        "    elif i < len(sentence_lst) - context_length:\n",
        "      context = sentence_lst[i-context_length:i] + sentence_lst[i+1:i+context_length+1]\n",
        "      skip_grams.append((val, context))\n",
        "    # returning the skip_grams\n",
        "  return skip_grams\n",
        "# calling the function \n",
        "skip_grams = get_skip_grams(sentence_lst, context)\n",
        "# printing out the results\n",
        "for skip_gram in skip_grams:\n",
        "    print(skip_gram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "To use CBOW or continuous bag of words, this goes through each word in a training corupus and tries to predict the word that goes after it by applying the a bow model to the words around it\n",
            "('or', ['to', 'use', 'cbow', 'continuous', 'bag', 'of'])\n",
            "('continuous', ['use', 'cbow', 'or', 'bag', 'of', 'words,'])\n",
            "('bag', ['cbow', 'or', 'continuous', 'of', 'words,', 'this'])\n",
            "('of', ['or', 'continuous', 'bag', 'words,', 'this', 'goes'])\n",
            "('words,', ['continuous', 'bag', 'of', 'this', 'goes', 'through'])\n",
            "('this', ['bag', 'of', 'words,', 'goes', 'through', 'each'])\n",
            "('goes', ['of', 'words,', 'this', 'through', 'each', 'word'])\n",
            "('through', ['words,', 'this', 'goes', 'each', 'word', 'in'])\n",
            "('each', ['this', 'goes', 'through', 'word', 'in', 'a'])\n",
            "('word', ['goes', 'through', 'each', 'in', 'a', 'training'])\n",
            "('in', ['through', 'each', 'word', 'a', 'training', 'corupus'])\n",
            "('a', ['each', 'word', 'in', 'training', 'corupus', 'and'])\n",
            "('training', ['word', 'in', 'a', 'corupus', 'and', 'tries'])\n",
            "('corupus', ['in', 'a', 'training', 'and', 'tries', 'to'])\n",
            "('and', ['a', 'training', 'corupus', 'tries', 'to', 'predict'])\n",
            "('tries', ['training', 'corupus', 'and', 'to', 'predict', 'the'])\n",
            "('to', ['corupus', 'and', 'tries', 'predict', 'the', 'word'])\n",
            "('predict', ['and', 'tries', 'to', 'the', 'word', 'that'])\n",
            "('the', ['tries', 'to', 'predict', 'word', 'that', 'goes'])\n",
            "('word', ['to', 'predict', 'the', 'that', 'goes', 'after'])\n",
            "('that', ['predict', 'the', 'word', 'goes', 'after', 'it'])\n",
            "('goes', ['the', 'word', 'that', 'after', 'it', 'by'])\n",
            "('after', ['word', 'that', 'goes', 'it', 'by', 'applying'])\n",
            "('it', ['that', 'goes', 'after', 'by', 'applying', 'the'])\n",
            "('by', ['goes', 'after', 'it', 'applying', 'the', 'a'])\n",
            "('applying', ['after', 'it', 'by', 'the', 'a', 'bow'])\n",
            "('the', ['it', 'by', 'applying', 'a', 'bow', 'model'])\n",
            "('a', ['by', 'applying', 'the', 'bow', 'model', 'to'])\n",
            "('bow', ['applying', 'the', 'a', 'model', 'to', 'the'])\n",
            "('model', ['the', 'a', 'bow', 'to', 'the', 'words'])\n",
            "('to', ['a', 'bow', 'model', 'the', 'words', 'around'])\n",
            "('the', ['bow', 'model', 'to', 'words', 'around', 'it'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e7GGl1lzH86"
      },
      "source": [
        "## Generating Word2vecs\n",
        "\n",
        "This can be done using the gensim package that allows the user to make a unique model\n",
        "\n",
        "### Notes on the arugments for model [generation](https://radimrehurek.com/gensim/models/word2vec.html)\n",
        "\n",
        "* size = is the number of embeddings/vector positions you want the  model to have for each word\n",
        "* window = the context window, max distance between one word to another\n",
        "* min_count = The amount of times a word has to appear to be apart of the corpus\n",
        "* workers = Thread count\n",
        "* sample = range(0,.00001) used for downsampleing\n",
        "* alpha = learning rate\n",
        "* min_alpha = what the learning rate will slow down to\n",
        "* sg = 1 for skipgrams, 0 for CBOW\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsfATLUV0epa",
        "outputId": "78914422-01d8-436e-e9fb-6ecc3bb9e28d"
      },
      "source": [
        "# Using the poems by E \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "#  loading in the poems\n",
        "example_poems = poems\n",
        "# processing the text\n",
        "procecess_poems =  [preprocess_text(poem, True) for poem in example_poems]\n",
        "\n",
        "# -- LETS BUILD THIS SHIT! -- #\n",
        "import gensim\n",
        "\n",
        "# make the model\n",
        "model_sg = gensim.models.Word2Vec(procecess_poems, window=5, min_count=1, workers=2, sg=1)\n",
        "model_CBOW = gensim.models.Word2Vec(procecess_poems, window=5, min_count=1, workers=2, sg=0)\n",
        "\n",
        "# saving the vocab of the model\n",
        "vocab_sg =  list(model_sg.wv.vocab)\n",
        "vocab_cbow = list(model_CBOW.wv.vocab)\n",
        "# view vocab from models (should be the same)\n",
        "print(vocab_sg)\n",
        "print(vocab_cbow)\n",
        "# Looking at the most similar to frog from both versions\n",
        "similar_to_frog_sg = model_sg.wv.most_similar(\"frog\", topn = 10)\n",
        "similar_to_frog_cbow = model_CBOW.wv.most_similar(\"frog\", topn = 10)\n",
        "# looking at the words\n",
        "print(similar_to_frog_sg, \"\\n\", similar_to_frog_cbow)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "['success', 'count', 'sweet', 'ne', 'er', 'succeed', 'comprehend', 'nectar', 'require', 'sorest', 'need', 'one', 'purple', 'host', 'take', 'flag', 'day', 'tell', 'definition', 'clear', 'victory', 'defeat', 'die', 'whose', 'forbid', 'ear', 'distant', 'strain', 'triumph', 'break', 'agonize', 'wild', 'night', 'thee', 'luxury', 'futile', 'wind', 'heart', 'port', 'do', 'compass', 'chart', 'rowing', 'eden', 'ah', 'sea', 'might', 'moor', 'nobody', 'pair', 'u', 'banish', 'know', 'dreary', 'somebody', 'public', 'like', 'frog', 'name', 'livelong', 'admire', 'bog', 'felt', 'funeral', 'brain', 'mourner', 'fro', 'keep', 'tread', 'till', 'seem', 'sense', 'seat', 'service', 'drum', 'beat', 'think', 'mind', 'go', 'numb', 'hear', 'lift', 'box', 'creak', 'across', 'soul', 'boot', 'lead', 'space', 'begin', 'toll', 'heaven', 'bell', 'silence', 'strange', 'race', 'wreck', 'solitary', 'hope', 'thing', 'feather', 'perch', 'sing', 'tune', 'without', 'word', 'never', 'stop', 'gale', 'sore', 'must', 'storm', 'could', 'abash', 'little', 'bird', 'many', 'warm', 'chillest', 'land', 'yet', 'extremity', 'ask', 'crumb', 'pedigree', 'honey', 'concern', 'bee', 'clover', 'time', 'aristocracy']\n",
            "['success', 'count', 'sweet', 'ne', 'er', 'succeed', 'comprehend', 'nectar', 'require', 'sorest', 'need', 'one', 'purple', 'host', 'take', 'flag', 'day', 'tell', 'definition', 'clear', 'victory', 'defeat', 'die', 'whose', 'forbid', 'ear', 'distant', 'strain', 'triumph', 'break', 'agonize', 'wild', 'night', 'thee', 'luxury', 'futile', 'wind', 'heart', 'port', 'do', 'compass', 'chart', 'rowing', 'eden', 'ah', 'sea', 'might', 'moor', 'nobody', 'pair', 'u', 'banish', 'know', 'dreary', 'somebody', 'public', 'like', 'frog', 'name', 'livelong', 'admire', 'bog', 'felt', 'funeral', 'brain', 'mourner', 'fro', 'keep', 'tread', 'till', 'seem', 'sense', 'seat', 'service', 'drum', 'beat', 'think', 'mind', 'go', 'numb', 'hear', 'lift', 'box', 'creak', 'across', 'soul', 'boot', 'lead', 'space', 'begin', 'toll', 'heaven', 'bell', 'silence', 'strange', 'race', 'wreck', 'solitary', 'hope', 'thing', 'feather', 'perch', 'sing', 'tune', 'without', 'word', 'never', 'stop', 'gale', 'sore', 'must', 'storm', 'could', 'abash', 'little', 'bird', 'many', 'warm', 'chillest', 'land', 'yet', 'extremity', 'ask', 'crumb', 'pedigree', 'honey', 'concern', 'bee', 'clover', 'time', 'aristocracy']\n",
            "[('go', 0.31624090671539307), ('seat', 0.2415621280670166), ('flag', 0.23062445223331451), ('forbid', 0.23039062321186066), ('aristocracy', 0.2091548591852188), ('across', 0.16420921683311462), ('ne', 0.16366320848464966), ('die', 0.15793658792972565), ('box', 0.1428433209657669), ('succeed', 0.1383267045021057)] \n",
            " [('go', 0.31259676814079285), ('seat', 0.2385214865207672), ('flag', 0.2291274219751358), ('forbid', 0.22445908188819885), ('aristocracy', 0.20912212133407593), ('ne', 0.1634349673986435), ('across', 0.16236120462417603), ('die', 0.15797902643680573), ('box', 0.13950468599796295), ('banish', 0.13724081218242645)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}